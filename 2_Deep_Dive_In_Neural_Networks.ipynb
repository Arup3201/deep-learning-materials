{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy2umik0W5XRfUX1BGeyxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/deep-learning-materials/blob/main/2_Deep_Dive_In_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Step 1: Introduction to Neural Networks**\n",
        "  * Briefly recap the concept of a single neuron and its function (inputs, weights, bias, output).\n",
        "  * Relate the idea of a single neuron to a linear regression problem they solved in the previous class.\n",
        "  * Introduce the concept of activation functions and why they are crucial in neural networks.\n",
        "  * Mention the role of non-linearity in activation functions to solve more complex problems.\n",
        "\n",
        "* **Step 2: Building Blocks of a Deep Neural Network**\n",
        "  * Discuss how a neural network is constructed using multiple layers of neurons.\n",
        "  * Emphasize the input layer, hidden layers, and the output layer.\n",
        "  * Relate the Dense layer they used in the regression problem to the concept of a fully connected layer.\n",
        "  * Introduce the concept of weights and biases in each layer.\n",
        "\n",
        "* **Step 3: Understanding Forward Propagation**\n",
        "  * Explain the forward propagation process in neural networks.\n",
        "  * Show how the output of one layer becomes the input to the next layer.\n",
        "  * Provide an intuitive example of how the data flows through the network.\n",
        "\n",
        "* **Step 4: Activation Functions**\n",
        "  * Dive deeper into activation functions like ReLU (Rectified Linear Unit), Sigmoid, and Tanh.\n",
        "  * Discuss the pros and cons of each activation function.\n",
        "  * Illustrate how activation functions introduce non-linearity to the model.\n",
        "  \n",
        "* **Step 5: Deep Neural Network for Regression**\n",
        "  * Demonstrate how to build a deep neural network for a regression problem using TensorFlow Keras.\n",
        "  * Connect this to their previous experience with the basic regression problem, highlighting the differences.\n",
        "  * Train the model on a simple regression dataset and evaluate its performance.\n",
        "\n",
        "* **Step 6: Introduction to Backpropagation**\n",
        "  * Explain the need for backpropagation to optimize the neural network.\n",
        "  * Connect backpropagation to the gradient descent algorithm for weight updates.\n",
        "  * Use an analogy or visual aid to make backpropagation easier to understand.\n",
        "\n",
        "* **Step 7: Training the Neural Network**\n",
        "  * Discuss the training process with the help of a loss/cost function and optimization algorithm.\n",
        "  * Introduce the concept of epochs and batch size during training.\n",
        "  * Show how the model learns from the data and improves over iterations.\n",
        "\n",
        "* **Step 8: Overfitting and Regularization**\n",
        "  * Highlight the problem of overfitting in deep neural networks.\n",
        "  * Introduce regularization techniques like L1 and L2 regularization.\n",
        "  * Explain how regularization helps in improving generalization.\n",
        "\n",
        "* **Step 9: Recap and Next Steps**\n",
        "  * Summarize the key concepts covered in the lecture.\n",
        "  * Encourage students to explore more complex neural network architectures like CNNs and RNNs.\n",
        "  * Mention the practical applications of neural networks in various fields."
      ],
      "metadata": {
        "id": "OnH9M-pAJ2Ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous notebook, you saw how to create a simple neural network to solve a regression problem. Regression problems are those problems where you need to output a real-number which has infinite possibilities. It can be 2, it can be 10.4, it can be 1000 or even a really big number. When your output can be any value from the infinite possible values then it is called regression.\n",
        "\n",
        "The relation in the previous notebook was `Y=2X-1` given the X and Y pairs.\n",
        "\n",
        "[![relation-X-Y.png](https://i.postimg.cc/RCkmrgYt/relation-X-Y.png)](https://postimg.cc/RqLyKL4C)\n",
        "\n",
        "Now, it is time to understand what happened at the backend properly.\n",
        "* What are neurons?\n",
        "* What are activation functions?\n",
        "* What are loss?\n",
        "* How optimizer works?\n",
        "* And other questions...\n",
        "\n",
        "\n",
        "The neural network does the same job as deriving the equation from the example dataset given to it when called the `fit(X, Y, epochs=...)` method. The value of `w` and `b` are something the neural network learns by itself by going through trial and error just like we humans do.\n",
        "\n",
        "[![X-Y-neural-network-representation.png](https://i.postimg.cc/xd9zLck1/X-Y-neural-network-representation.png)](https://postimg.cc/rDYskyCB)\n",
        "\n",
        "\n",
        "After it finds out the values of `w` and `b` then the only job remaining is simply put those values in the equation `wx+b`. When we call `predict([...])` method, the neural network simply puts the `x` value inside the equation `2x-1` and gives out the result.\n",
        "\n",
        "This eliptical shape that contains the equation is called `neuron` which does all the calculation and giving outputs.\n",
        "\n",
        "To explain the neuron in a more general way, we can take the following diagram.\n",
        "\n",
        "[![neural-network-neuron-breakdown.png](https://i.postimg.cc/h46rdjvT/neural-network-neuron-breakdown.png)](https://postimg.cc/PNbYGdm5)\n",
        "\n",
        "In this diagram as you can see the overall neuron has 2 parts -\n",
        "* One part calculates the `wx+b`, the result of which we save at `z`.\n",
        "* Other part is the `f(z)` which applies a function on `z`. `f` can be any function like linear, sigmoid, softmax, relu etc. We will look at different types of functions later. Most of the times `f` is linear by default. Linear function means, `f(z)=z` which is the case in our first and second example of neural network.\n",
        "\n",
        "We combine this 2 parts and say that the circle is the `neuron` given below,\n",
        "\n",
        "[![neural-network-img.png](https://i.postimg.cc/sxmhfsn6/neural-network-img.png)](https://postimg.cc/dZ714Pcr)\n",
        "\n",
        "**`f` is called the activation function.**\n",
        "\n",
        "\n",
        "What if we have more than 1 input?\n",
        "In that case we can take `w1`, `w2` and `b` to have the equation like - $$y = w1*x1 + w2*x2 + b$$\n",
        "\n",
        "But there is a better representation than this -\n",
        "$$y = w^T.x+b$$\n",
        "\n",
        "where $$\n",
        "w^T =\n",
        "\\begin{bmatrix}\n",
        "w1 & w2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "x =\n",
        "\\begin{bmatrix}\n",
        "x1 \\\\\n",
        "x2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Refer to [this website](http://matrixmultiplication.xyz/) to learn more about how matrix multiplication works.\n",
        "\n",
        "$$\n",
        "w^T.x+b \\\\\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "w1 & w2\n",
        "\\end{bmatrix}.\n",
        "\\begin{bmatrix}\n",
        "x1 \\\\\n",
        "x2\n",
        "\\end{bmatrix} + b \\\\\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "w1*x1 + w2*x2 + b\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Neural Network with 3 inputs -\n",
        "[![neural-network-img-1.png](https://i.postimg.cc/1zyJxf2k/neural-network-img-1.png)](https://postimg.cc/XrDwd7VL)\n",
        "\n",
        "In this whole network there can be multiple numbers of weights `w` and biases `b` for deep nueral nets. But, that will be discussed later.\n",
        "\n",
        "Now that you are familiar with neurons and activation functions, it is time to proceed deeper into this.\n",
        "\n",
        "I gave the answer to the questions of neuron and activation function. Now, what is loss and how optimizers improve the neural network? This are the questions I will answer.\n",
        "\n",
        "Loss will come into picture when we have the `y` or the final output. Here, the final output is definitely `y` because there is no other calculations need to be done on `y`.\n",
        "\n",
        "After we have the `y`, we find out how good or how bad the `y` or the final output is compared to the original given output. Remember when we called the `fit(xs, ys, epochs=...)` method we not only provided `xs` but also `ys`.\n",
        "\n",
        "Those `ys` will be used to compare the current neural networks output with the original one and then it will give us some value which will tell us how much badly our model is performing on the training data, this is known as `loss`.\n",
        "\n",
        "`loss` is nothing but a simple function which has an equation that contains output `y` and original `y` from the examples. For simplicity of reference, I will refer the output `y` as `yÌ‚` and original `y` from the examples simply as `y`.\n"
      ],
      "metadata": {
        "id": "M1EEMQGDluwq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjpRbGHAxKZg"
      },
      "outputs": [],
      "source": []
    }
  ]
}